{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5701dfeb-e145-4d3c-9fb5-db60b8650e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from numba import jit\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# sns.set()\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0c13c-f130-4afe-bea4-dc6a090f74f0",
   "metadata": {},
   "source": [
    "## train, val_seen and val_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42eeb4d8-e719-433b-ba81-81ccde3ad43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"train\",\"val_seen\",\"val_unseen\"]\n",
    "ep = {}\n",
    "gt = {}\n",
    "for split in splits:\n",
    "    sub_data_path = \"../data/datasets/R2R_VLNCE_FSASub/%s/%s_sub.json.gz\"%(split,split)\n",
    "    with gzip.open(sub_data_path, \"r\") as f:\n",
    "        ep[split] = json.loads(f.read())\n",
    "    sub_gt_path = \"../data/datasets/R2R_VLNCE_FSASub/%s/%s_gt.json.gz\"%(split,split)\n",
    "    with gzip.open(sub_gt_path, \"r\") as f:\n",
    "        gt[split] = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8662e4c-b0d0-448d-b366-b6186616d97d",
   "metadata": {},
   "source": [
    "need shift episode id. \n",
    "keep trajectory id\n",
    "\n",
    "ground truth use episode id to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e399ea-3d93-43f1-8cd9-5d7074d97de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_id = {}\n",
    "for split in splits:\n",
    "    ids = []\n",
    "    for v in ep[split][\"episodes\"]:\n",
    "        ids.append(v[\"episode_id\"])\n",
    "    ep_id[split] = ids\n",
    "tr_id = {}\n",
    "for split in splits:\n",
    "    ids = []\n",
    "    for v in ep[split][\"episodes\"]:\n",
    "        ids.append(v[\"trajectory_id\"])\n",
    "    tr_id[split] = ids\n",
    "gt_id = {}\n",
    "for split in splits:\n",
    "    ids = []\n",
    "    for k,v in gt[split].items():\n",
    "        ids.append(int(k))\n",
    "    gt_id[split] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89208d3c-e02c-4db5-93be-99d368ce637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10837\n",
      "11615\n"
     ]
    }
   ],
   "source": [
    "train_val_ep = []\n",
    "train_val_gt = {}\n",
    "\n",
    "for v in ep[\"train\"][\"episodes\"]:\n",
    "    train_val_ep.append(v)\n",
    "for k,v in gt[\"train\"].items():\n",
    "    train_val_gt[k]=v\n",
    "    \n",
    "shift=max(ep_id[\"train\"])\n",
    "print(shift)\n",
    "for v in ep[\"val_seen\"][\"episodes\"]:\n",
    "    v[\"episode_id\"]+=shift\n",
    "    train_val_ep.append(v)\n",
    "for k,v in gt[\"val_seen\"].items():\n",
    "    k = str(int(k)+shift)\n",
    "    assert k not in train_val_gt\n",
    "    train_val_gt[k]=v\n",
    "\n",
    "shift=max(ep_id[\"train\"])+max(ep_id[\"val_seen\"])\n",
    "print(shift)\n",
    "for v in ep[\"val_unseen\"][\"episodes\"]:\n",
    "    v[\"episode_id\"]+=shift\n",
    "    train_val_ep.append(v)\n",
    "for k,v in gt[\"val_unseen\"].items():\n",
    "    k = str(int(k)+shift)\n",
    "    assert k not in train_val_gt\n",
    "    train_val_gt[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac45a18-5da1-40c2-9b0e-1351a982f4b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '../data/datasets/R2R_VLNCE_FSASub/train_val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m os\u001b[39m.\u001b[39;49mmakedirs(\u001b[39m\"\u001b[39;49m\u001b[39m../data/datasets/R2R_VLNCE_FSASub/train_val\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/habitat/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '../data/datasets/R2R_VLNCE_FSASub/train_val'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/datasets/R2R_VLNCE_FSASub/train_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c5d05c-04c5-429e-82d7-4154cd8e11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"episodes\":train_val_ep, \"instruction_vocab\":ep[\"train\"][\"instruction_vocab\"]}\n",
    "sub_data_path = \"../data/datasets/R2R_VLNCE_FSASub/train_val/train_val_sub.json.gz\"\n",
    "with gzip.open(sub_data_path, \"w\") as f:\n",
    "    f.write(json.dumps(data).encode(\"utf-8\"))\n",
    "sub_gt_path = \"../data/datasets/R2R_VLNCE_FSASub/train_val/train_val_gt.json.gz\"\n",
    "with gzip.open(sub_gt_path, \"w\") as f:\n",
    "    f.write(json.dumps(train_val_gt).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22591ae4-3eca-4447-9780-9e02dd709890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13436"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f521d8-67f2-4a23-b768-6eef1293c5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2504"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ep[\"val_seen\"][\"instruction_vocab\"][\"word_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c22c1a27-0d0f-4e33-b2a4-a5849ebd419f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2597"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1819+778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9e10f7b-2a17-4f1e-b614-0de1700970fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5811388300841898"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1a554-b3c6-4cdb-8228-5e798da843e6",
   "metadata": {},
   "source": [
    "## val_seen and val_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9569f8a7-983e-4c8e-946b-ac85d8af6d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"val_seen\",\"val_unseen\"]\n",
    "ep = {}\n",
    "gt = {}\n",
    "for split in splits:\n",
    "    sub_data_path = \"../data/datasets/R2R_VLNCE_NRSub_T/%s/%s_sub.json.gz\"%(split,split)\n",
    "    with gzip.open(sub_data_path, \"r\") as f:\n",
    "        ep[split] = json.loads(f.read())\n",
    "    sub_gt_path = \"../data/datasets/R2R_VLNCE_NRSub_T/%s/%s_gt.json.gz\"%(split,split)\n",
    "    with gzip.open(sub_gt_path, \"r\") as f:\n",
    "        gt[split] = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03156a3e-ce5d-4f11-9b8a-3f1813552871",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_id = {}\n",
    "for split in splits:\n",
    "    ids = []\n",
    "    for v in ep[split][\"episodes\"]:\n",
    "        ids.append(v[\"episode_id\"])\n",
    "    ep_id[split] = ids\n",
    "tr_id = {}\n",
    "for split in splits:\n",
    "    ids = []\n",
    "    for v in ep[split][\"episodes\"]:\n",
    "        ids.append(v[\"trajectory_id\"])\n",
    "    tr_id[split] = ids\n",
    "gt_id = {}\n",
    "for split in splits:\n",
    "    ids = []\n",
    "    for k,v in gt[split].items():\n",
    "        ids.append(int(k))\n",
    "    gt_id[split] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e4ddeba-94fc-4fa2-b32b-3f289544347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778\n"
     ]
    }
   ],
   "source": [
    "train_val_ep = []\n",
    "train_val_gt = {}\n",
    "\n",
    "for v in ep[\"val_seen\"][\"episodes\"]:\n",
    "    train_val_ep.append(v)\n",
    "for k,v in gt[\"val_seen\"].items():\n",
    "    train_val_gt[k]=v\n",
    "\n",
    "shift=max(ep_id[\"val_seen\"])\n",
    "print(shift)\n",
    "for v in ep[\"val_unseen\"][\"episodes\"]:\n",
    "    v[\"episode_id\"]+=shift\n",
    "    train_val_ep.append(v)\n",
    "for k,v in gt[\"val_unseen\"].items():\n",
    "    k = str(int(k)+shift)\n",
    "    assert k not in train_val_gt\n",
    "    train_val_gt[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc4a78bd-29c6-4c5d-9c6b-436ca59856b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/datasets/R2R_VLNCE_NRSub_T/val_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9af0aba6-6c84-403d-8556-694e943e115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"episodes\":train_val_ep, \"instruction_vocab\":ep[\"val_seen\"][\"instruction_vocab\"]}\n",
    "sub_data_path = \"../data/datasets/R2R_VLNCE_NRSub_T/val_all/val_all_sub.json.gz\"\n",
    "with gzip.open(sub_data_path, \"w\") as f:\n",
    "    f.write(json.dumps(data).encode(\"utf-8\"))\n",
    "sub_gt_path = \"../data/datasets/R2R_VLNCE_NRSub_T/val_all/val_all_gt.json.gz\"\n",
    "with gzip.open(sub_gt_path, \"w\") as f:\n",
    "    f.write(json.dumps(train_val_gt).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef5c59-45e3-4a89-b99b-6476ee21dafb",
   "metadata": {},
   "source": [
    "## train seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc3153bf-cabd-4eae-abfe-094d62e7f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"val_seen\",\"train\"]\n",
    "ep = {}\n",
    "gt = {}\n",
    "for split in splits:\n",
    "    sub_data_path = \"../data/datasets/R2R_VLNCE_NRSub_T/%s/%s_sub.json.gz\"%(split,split)\n",
    "    with gzip.open(sub_data_path, \"r\") as f:\n",
    "        ep[split] = json.loads(f.read())\n",
    "    sub_gt_path = \"../data/datasets/R2R_VLNCE_NRSub_T/%s/%s_gt.json.gz\"%(split,split)\n",
    "    with gzip.open(sub_gt_path, \"r\") as f:\n",
    "        gt[split] = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02e7c7d9-7c5f-4271-b773-3e27e874497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_id = {}\n",
    "for split in splits:\n",
    "    ids = []\n",
    "    for v in ep[split][\"episodes\"]:\n",
    "        ids.append(v[\"episode_id\"])\n",
    "    ep_id[split] = ids\n",
    "tr_id = {}\n",
    "for split in splits:\n",
    "    ids = []\n",
    "    for v in ep[split][\"episodes\"]:\n",
    "        ids.append(v[\"trajectory_id\"])\n",
    "    tr_id[split] = ids\n",
    "gt_id = {}\n",
    "for split in splits:\n",
    "    ids = []\n",
    "    for k,v in gt[split].items():\n",
    "        ids.append(int(k))\n",
    "    gt_id[split] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4acef514-b979-4134-bd6c-aa9df009545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep[\"train\"][\"episodes\"] = random.sample(ep[\"train\"][\"episodes\"],1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "928b6d19-5b56-4610-bbf6-3c199f55ea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10837\n"
     ]
    }
   ],
   "source": [
    "train_val_ep = []\n",
    "train_val_gt = {}\n",
    "\n",
    "for v in ep[\"train\"][\"episodes\"]:\n",
    "    train_val_ep.append(v)\n",
    "for k,v in gt[\"train\"].items():\n",
    "    train_val_gt[k]=v\n",
    "\n",
    "shift=max(ep_id[\"train\"])\n",
    "print(shift)\n",
    "for v in ep[\"val_seen\"][\"episodes\"]:\n",
    "    v[\"episode_id\"]+=shift\n",
    "    train_val_ep.append(v)\n",
    "for k,v in gt[\"val_seen\"].items():\n",
    "    k = str(int(k)+shift)\n",
    "    assert k not in train_val_gt\n",
    "    train_val_gt[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e2a51ce-98cb-4686-a78f-f6d74d6d0a35",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '../data/datasets/R2R_VLNCE_NRSub_T/train_seen'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/datasets/R2R_VLNCE_NRSub_T/train_seen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/habitat/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '../data/datasets/R2R_VLNCE_NRSub_T/train_seen'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/datasets/R2R_VLNCE_NRSub_T/train_seen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f77d027d-1845-4e34-8093-190ef1b1f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"episodes\":train_val_ep, \"instruction_vocab\":ep[\"val_seen\"][\"instruction_vocab\"]}\n",
    "sub_data_path = \"../data/datasets/R2R_VLNCE_NRSub_T/train_seen/train_seen_sub.json.gz\"\n",
    "with gzip.open(sub_data_path, \"w\") as f:\n",
    "    f.write(json.dumps(data).encode(\"utf-8\"))\n",
    "sub_gt_path = \"../data/datasets/R2R_VLNCE_NRSub_T/train_seen/train_seen_gt.json.gz\"\n",
    "with gzip.open(sub_gt_path, \"w\") as f:\n",
    "    f.write(json.dumps(train_val_gt).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c50240b5-0b7d-4ac5-a95b-bf4944b93027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2278"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13d00301-f0e0-4ed6-9de6-394f837e649d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample([1,2,3,4,5,],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6557c-7982-4ff5-bca7-e0fb8d2eb2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "a09f9bb9480524953e8bc89da8dc6c89068fed2b02c1ccad0d5bd30167956ac4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
