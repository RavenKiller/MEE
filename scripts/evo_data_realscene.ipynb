{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import lmdb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm,trange\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import pprint\n",
    "from ip_basic import depth_map_utils\n",
    "# for nyu, tum, diode\n",
    "def center_crop(im, new_width=None, new_height=None):\n",
    "    width, height = im.size   # Get dimensions\n",
    "    left = (width - new_width)/2\n",
    "    top = (height - new_height)/2\n",
    "    right = (width + new_width)/2\n",
    "    bottom = (height + new_height)/2\n",
    "    return im.crop((left, top, right, bottom))\n",
    "def show_example(path='/hy-tmp/stage1/rgb_depth_large.mat', idx=0):\n",
    "    with  h5py.File(path,'r') as t:\n",
    "        rgb = t['rgb']\n",
    "        depth = t['depth']\n",
    "        print(rgb.shape)\n",
    "        rgbi = rgb[idx]\n",
    "        depthi = depth[idx]\n",
    "    fig=plt.figure()\n",
    "    ax1 = fig.add_subplot(111,projection='3d')\n",
    "    xx = np.arange(0,256)\n",
    "    yy = np.arange(0,256)\n",
    "    X, Y = np.meshgrid(xx, yy)\n",
    "    ax1.plot_surface(X,Y,depthi.squeeze(),cmap='rainbow')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    fig=plt.figure()\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax1.imshow(rgbi)\n",
    "    ax2.imshow(depthi)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# for tum\n",
    "def depth_completion(depthi, loop=2):\n",
    "    '''ip_basic depth completion\n",
    "    '''\n",
    "    fill_type = 'fast'\n",
    "    extrapolate = True\n",
    "    blur_type = 'bilateral'\n",
    "    for _ in range(loop):\n",
    "        if fill_type == 'fast':\n",
    "            depthi = depth_map_utils.fill_in_fast(\n",
    "                depthi, extrapolate=extrapolate, blur_type=blur_type)\n",
    "        elif fill_type == 'multiscale':\n",
    "            depthi, process_dict = depth_map_utils.fill_in_multiscale(\n",
    "                depthi, extrapolate=extrapolate, blur_type=blur_type,\n",
    "                show_process=False)\n",
    "    return depthi\n",
    "def get_timestamp(v):\n",
    "    f = v.split(\"/\")[-1]\n",
    "    stamp = f.replace(\".png\",\"\")\n",
    "    return float(stamp)\n",
    "def align_rgb_depth(rgbs, depths, bi=True):\n",
    "    '''Align two sequeces by timestamps in the filenames\n",
    "    Use rgb as base\n",
    "    '''\n",
    "    if len(rgbs)==len(depths):\n",
    "        return rgbs, depths\n",
    "    res_rgbs = rgbs\n",
    "    res_depths = []\n",
    "    for i, v in enumerate(res_rgbs):\n",
    "        target_stamp = get_timestamp(v)\n",
    "        current_d = depths[0]\n",
    "        if bi:\n",
    "            # bi-search\n",
    "            left = 0\n",
    "            right = len(depths)-1\n",
    "            while left<=right:\n",
    "                mid = (left+right)//2\n",
    "                now_stamp = get_timestamp(depths[mid])\n",
    "                if now_stamp==target_stamp:\n",
    "                    left=mid\n",
    "                    right=mid-1\n",
    "                elif now_stamp>target_stamp:\n",
    "                    right = mid-1\n",
    "                else:\n",
    "                    left = mid+1\n",
    "            if left==0:\n",
    "                current_d = depths[left]\n",
    "            elif right==len(depths)-1:\n",
    "                current_d = depths[right]\n",
    "            else:\n",
    "                current_d = depths[left] if abs(get_timestamp(depths[left])-target_stamp)<abs(get_timestamp(depths[left-1])-target_stamp) else depths[left-1]\n",
    "        else:\n",
    "            current_min = 100000000\n",
    "            for j, p in enumerate(depths):\n",
    "                current_stamp = get_timestamp(p)\n",
    "                if abs(target_stamp-current_stamp)<current_min:\n",
    "                    current_min =  abs(target_stamp-current_stamp)\n",
    "                    current_d = p\n",
    "        res_depths.append(current_d)\n",
    "    assert len(res_rgbs)==len(res_depths)\n",
    "    return res_rgbs, res_depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split episode and panorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/fuenwang/Equirec2Perspec\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Equirectangular:\n",
    "    def __init__(self, img):\n",
    "        self._img = img\n",
    "        [self._height, self._width, _] = self._img.shape\n",
    "        #cp = self._img.copy()  \n",
    "        #w = self._width\n",
    "        #self._img[:, :w/8, :] = cp[:, 7*w/8:, :]\n",
    "        #self._img[:, w/8:, :] = cp[:, :7*w/8, :]\n",
    "    \n",
    "\n",
    "    def GetPerspective(self, FOV, THETA, PHI, height, width, RADIUS = 128):\n",
    "        #\n",
    "        # THETA is left/right angle, PHI is up/down angle, both in degree\n",
    "        #\n",
    "\n",
    "        equ_h = self._height\n",
    "        equ_w = self._width\n",
    "        equ_cx = (equ_w - 1) / 2.0\n",
    "        equ_cy = (equ_h - 1) / 2.0     \n",
    "        \n",
    "        hFOV = math.radians(FOV)\n",
    "        wFOV = 2*math.atan(float(width)/height * np.tan(hFOV / 2.0))\n",
    "\n",
    "        c_x = (width - 1) / 2.0\n",
    "        c_y = (height - 1) / 2.0\n",
    "\n",
    "        wangle = (math.pi - wFOV) / 2.0\n",
    "        w_len = 2 * RADIUS * np.sin(wFOV / 2.0) / np.sin(wangle)\n",
    "        w_interval = w_len / (width - 1)\n",
    "\n",
    "        hangle = (math.pi - hFOV) / 2.0\n",
    "        h_len = 2 * RADIUS * np.sin(hFOV / 2.0) / np.sin(hangle)\n",
    "        h_interval = h_len / (height - 1)\n",
    "        x_map = np.zeros([height, width], np.float32) + RADIUS\n",
    "        y_map = np.tile((np.arange(0, width) - c_x) * w_interval, [height, 1])\n",
    "        z_map = -np.tile((np.arange(0, height) - c_y) * h_interval, [width, 1]).T\n",
    "        D = np.sqrt(x_map**2 + y_map**2 + z_map**2)\n",
    "        xyz = np.zeros([height, width, 3], float)\n",
    "        xyz[:, :, 0] = (RADIUS / D * x_map)[:, :]\n",
    "        xyz[:, :, 1] = (RADIUS / D * y_map)[:, :]\n",
    "        xyz[:, :, 2] = (RADIUS / D * z_map)[:, :]\n",
    "        \n",
    "        y_axis = np.array([0.0, 1.0, 0.0], np.float32)\n",
    "        z_axis = np.array([0.0, 0.0, 1.0], np.float32)\n",
    "        [R1, _] = cv2.Rodrigues(z_axis * np.radians(THETA))\n",
    "        [R2, _] = cv2.Rodrigues(np.dot(R1, y_axis) * np.radians(-PHI))\n",
    "\n",
    "        xyz = xyz.reshape([height * width, 3]).T\n",
    "        xyz = np.dot(R1, xyz)\n",
    "        xyz = np.dot(R2, xyz).T\n",
    "        lat = np.arcsin(xyz[:, 2] / RADIUS)\n",
    "        lon = np.zeros([height * width], float)\n",
    "        theta = np.arctan(xyz[:, 1] / xyz[:, 0])\n",
    "        idx1 = xyz[:, 0] > 0\n",
    "        idx2 = xyz[:, 1] > 0\n",
    "\n",
    "        idx3 = ((1 - idx1) * idx2).astype(bool)\n",
    "        idx4 = ((1 - idx1) * (1 - idx2)).astype(bool)\n",
    "        \n",
    "        lon[idx1] = theta[idx1]\n",
    "        lon[idx3] = theta[idx3] + np.pi\n",
    "        lon[idx4] = theta[idx4] - np.pi\n",
    "\n",
    "        lon = lon.reshape([height, width]) / np.pi * 180\n",
    "        lat = -lat.reshape([height, width]) / np.pi * 180\n",
    "        lon = lon / 180 * equ_cx + equ_cx\n",
    "        lat = lat / 90 * equ_cy + equ_cy\n",
    "        #for x in range(width):\n",
    "        #    for y in range(height):\n",
    "        #        cv2.circle(self._img, (int(lon[y, x]), int(lat[y, x])), 1, (0, 255, 0))\n",
    "        #return self._img \n",
    "    \n",
    "        persp = cv2.remap(self._img, lon.astype(np.float32), lat.astype(np.float32), cv2.INTER_CUBIC, borderMode=cv2.BORDER_WRAP)\n",
    "        return persp\n",
    "\n",
    "\"\"\"从全景图得到子图的代码\"\"\"\n",
    "# Simulator image parameters\n",
    "VIEWPOINT_SIZE = 36\n",
    "FEATURE_SIZE = 2048\n",
    "\n",
    "# WIDTH = 480\n",
    "# HEIGHT = 480\n",
    "# VFOV = 90\n",
    "# HFOV = 90  # makes a VFOV of 60\n",
    "\n",
    "def equirectangular_to_perspective(image, save_path, viewpoint):\n",
    "    ''' Convert an equirectangular pano to 36 perspective images used by the VLN\n",
    "        agent. Construct it with 12 heading \n",
    "        viewpoints and 3 elevation viewpoints, starting with the heading that is\n",
    "        directly in front. The agent works better if these views are aligned to the\n",
    "        building. '''\n",
    "    equi = Equirectangular(image)  # RGB shape of [H, W, 3]\n",
    "    im_width = 224\n",
    "    im_height = 224\n",
    "    vfov = 90\n",
    "\n",
    "    # heading, elevation. Here, heading is defined from the x-axis, and turning right \n",
    "    # is positive. This matches the matterport sim, but opposite to ros\n",
    "    he = np.zeros((3, 12, 2), dtype=np.float32)\n",
    "\n",
    "    \n",
    "    # for i,v_deg in enumerate(range(0, 30, 30)):\n",
    "        # 注意！ 这个-150°是和全景图在MP3D模拟器中的航向角挂钩的，MP3D中的航向角=150 - ros_heading\n",
    "    v_deg = 0\n",
    "    i = 0\n",
    "    for j,u_deg in enumerate(range(-180, 180, 90)):\n",
    "        # Negate heading since for e2p turning right is positive, opposite in ros\n",
    "        head = -math.radians(u_deg)\n",
    "        he[i,j] = [head + 2*math.pi if head<0 else head, math.radians(v_deg)]\n",
    "        p_im = equi.GetPerspective(vfov, u_deg, v_deg, im_height, im_width)\n",
    "        cv2.imwrite(os.path.join(save_path, f\"{viewpoint}_{str(12*i+j)}.jpg\"), p_im)\n",
    "           \n",
    "def roll_image(image, pano_heading_rad):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        image: theta相机获得的全景图,机器人坐标系\n",
    "        pano_heading_rad: 机器人朝向\n",
    "    output:\n",
    "        image: 将全景图转到地图坐标系0方向\n",
    "    \"\"\"\n",
    "    x_axis = 1 # width\n",
    "    roll_pixels = -int(pano_heading_rad / (math.pi * 2) * image.shape[x_axis])\n",
    "    rolled_image = np.roll(image, roll_pixels, axis=x_axis)\n",
    "    return rolled_image           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50707cf18ab479a9b9c71563dff4822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data_path = \"/root/autodl-tmp/r2r/\"\n",
    "dataset_path = \"/root/autodl-tmp/r2r/tjark_r2r_all.json\"\n",
    "new_data_path = \"/root/autodl-tmp/r2r_split/\"\n",
    "os.makedirs(new_data_path, exist_ok=True)\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    tjark_r2r = json.loads(f.read())\n",
    "for i, ep in tqdm(enumerate(tjark_r2r)):\n",
    "    scan = ep[\"scan\"]+\"_imgs\"\n",
    "    path = ep[\"path\"]\n",
    "    inst = ep[\"instructions\"][0]\n",
    "\n",
    "    os.makedirs(os.path.join(new_data_path, str(i)), exist_ok=True)\n",
    "    inst_path = os.path.join(new_data_path, str(i), \"instruction.txt\")\n",
    "    with open(inst_path,\"w\") as f:\n",
    "        f.write(inst)\n",
    "\n",
    "    for j,vp in enumerate(path):\n",
    "        img_path = os.path.join(raw_data_path, scan, vp+\"_equirectangular.jpg\")\n",
    "        img = cv2.imread(img_path)\n",
    "        os.makedirs(os.path.join(new_data_path, str(i), \"rgb\"), exist_ok=True)\n",
    "        equirectangular_to_perspective(img, os.path.join(new_data_path, str(i), \"rgb\"), j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/isl-org_ZoeDepth_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_size [384, 512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params passed to Resize transform:\n",
      "\twidth:  512\n",
      "\theight:  384\n",
      "\tresize_target:  True\n",
      "\tkeep_aspect_ratio:  True\n",
      "\tensure_multiple_of:  32\n",
      "\tresize_method:  minimal\n",
      "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_NK.pt\n",
      "Loaded successfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0490e63b6534c13bd68af093aa0c7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# timm==0.6.7\n",
    "# \n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "resize = transforms.Resize([384, 512])\n",
    "scale = 4.0\n",
    "\n",
    "test_image = 'C:/Users/tangjg/Desktop/08c774f20c984008882da2b8547850eb_skybox1_sami.jpg'\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cpu\":\n",
    "    print(\"WARNING: Running on CPU. This will be slow. Check your CUDA installation.\")\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # torch.hub.help(\"intel-isl/MiDaS\", \"DPT_BEiT_L_384\", force_reload=True) \n",
    "    model = torch.hub.load(\"isl-org/ZoeDepth\", \"ZoeD_NK\", pretrained=True)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def InferImage(image_file):\n",
    "    image = Image.open(image_file)\n",
    "    origin_size = image.size\n",
    "    img = ToTensor()(image).to(DEVICE).unsqueeze_(0)\n",
    "    img.to(DEVICE)\n",
    "    \n",
    "    model = load_model()\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(img)\n",
    "        out_img = Image.fromarray(out['metric_depth'].squeeze().cpu().numpy())\n",
    "        out_img.resize(origin_size, Image.ANTIALIAS)\n",
    "        out_img.show()\n",
    "\n",
    "model = load_model()\n",
    "def inferImageAndCompare(idx, model):\n",
    "    rgb_path = f'/root/autodl-tmp/r2r_split/{idx}/rgb'\n",
    "    out_path = f'/root/autodl-tmp/r2r_split/{idx}/depth'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    rgb_images = os.listdir(rgb_path)\n",
    "    for rgb_img in rgb_images:\n",
    "        path = os.path.join(rgb_path, rgb_img)\n",
    "        \n",
    "        image = Image.open(path)\n",
    "        origin_size = image.size\n",
    "        origin_max = np.array(image).max()\n",
    "        \n",
    "        depth_pil = model.infer_pil(image, output_type='pil')\n",
    "        \n",
    "        # resize并进行scale缩放，与GT尺度相同\n",
    "        depth_pil = depth_pil.resize(origin_size, Image.LANCZOS)\n",
    "        out_arr = np.array(depth_pil)\n",
    "        out_arr = out_arr*scale # multiplied by 4, assume it is accurate absolute depth*1000\n",
    "        depth_pil = Image.fromarray(out_arr.astype(np.uint16))\n",
    "        \n",
    "        depth_pil.save(os.path.join(out_path, rgb_img.replace(\".jpg\",\".png\")))\n",
    "for i in tqdm(range(408)):\n",
    "    inferImageAndCompare(i, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "from collections import Counter\n",
    "import glob\n",
    "\n",
    "import h5py\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from numba import jit\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import pathlib\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "random.seed(25)\n",
    "np.random.seed(25)\n",
    "def instruction_cut_clip(episodes, append_dot=False, keep_subs=True, refine=False, split_func=None, lower_all=False):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        episodes: a dataset list, [{\"instruction_text\":\"\"}]\n",
    "        append_dot: whether to add a \".\" at the end of sub-instructions\n",
    "        keep_subs: whether to keep sub-instruction text after processing\n",
    "        refine: whether to use proposed refine processing\n",
    "        split_func: the function used to cut instructions, default is `nltk.sent_tokenize`\n",
    "        lower_all: whether to lower all characters in sub-instructions.\n",
    "    Return:\n",
    "        train_data with \"sub_instruction_tokens\" and \"sub_instruction\"\n",
    "    \"\"\"\n",
    "    if split_func is None:\n",
    "        split_func = nltk.sent_tokenize\n",
    "    print(split_func)\n",
    "    train_data = copy.deepcopy(episodes)\n",
    "    # pre process\n",
    "    char_pattern = re.compile(r\"[a-zA-Z]\")\n",
    "    for i, item in enumerate(tqdm(train_data)):\n",
    "        inst = item[\"instruction_text\"]\n",
    "        inst = inst.strip()\n",
    "        start_idx = 0\n",
    "        while not char_pattern.search(inst[start_idx]):\n",
    "            start_idx += 1\n",
    "        inst = inst[start_idx:]\n",
    "        if lower_all:\n",
    "            inst = inst.lower()\n",
    "        train_data[i][\"instruction_text\"] = inst.replace(\"...\", \".\").replace(\"..\", \".\").replace(\".\",\". \").replace(\"  \", \" \")\n",
    "    \n",
    "    # cut by nltk\n",
    "    pattern = re.compile(r\"\\r\\n\")\n",
    "    for i, item in enumerate(tqdm(train_data)):\n",
    "        inst = item[\"instruction_text\"]\n",
    "        res = []\n",
    "        now = pattern.split(inst)\n",
    "        for v in now:\n",
    "            res.extend(split_func(v))\n",
    "        train_data[i][\"sub_instruction\"] = [piece.strip() for piece in res if piece.strip()]\n",
    "    # refine\n",
    "    if refine:\n",
    "        punctuation_list = [\",\", \".\"]\n",
    "        char_pattern = re.compile(r\"[a-zA-Z]+\")\n",
    "        def judge_verb(word):\n",
    "            const_verbs = [\"wait\", \"turn\", \"walk\", \"stop\"]\n",
    "            if \"VB\" in word[1]:\n",
    "                return True\n",
    "            if word[0] in const_verbs:\n",
    "                return True\n",
    "            return False\n",
    "        for i, item in enumerate(tqdm(train_data)):\n",
    "            new_sub = []\n",
    "            for k, piece in enumerate(item[\"sub_instruction\"]):\n",
    "                word_list = nltk.pos_tag(nltk.word_tokenize(piece))\n",
    "                tmp = \"\"\n",
    "                for x, word in enumerate(word_list):\n",
    "                    if (word[0].lower()==\"and\" or word[0]==\",\" or word[0].lower()==\"then\") and (x+1<len(word_list) and judge_verb(word_list[x+1])):\n",
    "                        if tmp and char_pattern.search(tmp):\n",
    "                            new_sub.append(tmp)\n",
    "                        if word[0].lower()==\"and\" or word[0].lower()==\"then\":\n",
    "                            tmp = word[0]\n",
    "                        else:\n",
    "                            tmp = \"\"\n",
    "                            \n",
    "                    elif (word[0]==\"and\" or word[0]==\",\") and (x+1<len(word_list) and word_list[x+1][0]==\"then\"):\n",
    "                        if tmp:\n",
    "                            new_sub.append(tmp)\n",
    "                        if word[0].lower()==\"and\" or word[0].lower()==\"then\":\n",
    "                            tmp = word[0]\n",
    "                        else:\n",
    "                            tmp = \"\"\n",
    "                    else:\n",
    "                        if not tmp or word[0] in punctuation_list:\n",
    "                            tmp+=word[0]\n",
    "                        else:\n",
    "                            tmp+=(\" \"+word[0])\n",
    "                if tmp:\n",
    "                    new_sub.append(tmp)\n",
    "            train_data[i][\"sub_instruction\"] = new_sub\n",
    "    \n",
    "    # post process and generate tokens\n",
    "    char_pattern = re.compile(r\"[a-zA-Z]\")\n",
    "    max_len = 0\n",
    "    pad_index = 0\n",
    "    pad_len = 86 # 0.09%\n",
    "    sub_pad_len = 77 # 0.05%\n",
    "    sub_num = 12 # 0.04%\n",
    "    useless_sub = [pad_index]*sub_pad_len\n",
    "    sub_split_index = -1\n",
    "    for i, item in enumerate(tqdm(train_data)):\n",
    "        tokens_all = []\n",
    "        tokens_split = []\n",
    "        for k, piece in enumerate(item[\"sub_instruction\"]):\n",
    "            piece = piece.strip()\n",
    "            assert piece\n",
    "            idx = len(piece)-1\n",
    "            while idx>=0 and piece[idx] in [\".\", \",\"]:\n",
    "                idx -= 1\n",
    "            if append_dot:\n",
    "                piece = piece[0:(idx+1)]+\".\"\n",
    "            else:\n",
    "                piece = piece[0:(idx+1)]\n",
    "            piece = piece.replace(\"``\", \"\\\"\").replace(\"''\", \"\\\"\")\n",
    "            train_data[i][\"sub_instruction\"][k] = piece\n",
    "            # piece_tokens = clip.tokenize(piece, truncate=True).squeeze(0).tolist()\n",
    "            # tokens_split.append(piece_tokens)\n",
    "        # if len(tokens_split)>sub_num:\n",
    "        #     tokens_split = tokens_split[0:sub_num]\n",
    "        # tokens_split.extend([useless_sub]*(sub_num-len(tokens_split)))\n",
    "        \n",
    "        # train_data[i][\"instruction_tokens\"] = clip.tokenize(item[\"instruction_text\"], truncate=True, context_length=77).squeeze(0).tolist()\n",
    "        # train_data[i][\"sub_instruction_tokens\"] = tokens_split\n",
    "        if not keep_subs:\n",
    "            del item[\"sub_instruction\"]\n",
    "    return train_data\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "def random_chunks(lst, left, right):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    i = 0\n",
    "    n = len(lst)\n",
    "    while i<n:\n",
    "        k = random.randint(left,right)\n",
    "        yield lst[i:min(i+k, n)]\n",
    "        i += k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function sent_tokenize at 0x7f04992ac550>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593314583bcb424280197895b508dd7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6f70a1797c40609516c2fcc9dc8a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b734fbc37a3b456ba12f28049dae5ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d35c7e989f9460597cebd978d973315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MIN_DEPTH = 0\n",
    "MAX_DEPTH = 10\n",
    "CROP_SIZE = 768\n",
    "RGB_SIZE = 224\n",
    "DEPTH_SIZE = 224\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "os.makedirs(\"/root/autodl-tmp/realscene\", exist_ok=True)\n",
    "real_folder = \"/root/autodl-tmp/realscene\"\n",
    "\n",
    "rgbs = []\n",
    "depths = []\n",
    "insts = []\n",
    "subs = []\n",
    "for ep_id in range(408):\n",
    "    episode_folder = \"/root/autodl-tmp/r2r_split/{}\".format(ep_id)\n",
    "    with open(os.path.join(episode_folder, \"instruction.txt\"), \"r\") as f:\n",
    "        inst = f.read()\n",
    "    rgbs.extend(sorted(glob.glob(os.path.join(episode_folder, \"rgb\",\"*\"))))\n",
    "    depths.extend(sorted(glob.glob(os.path.join(episode_folder, \"depth\",\"*\"))))\n",
    "    assert len(rgbs)==len(depths)\n",
    "    insts.extend([inst]*len(sorted(glob.glob(os.path.join(episode_folder, \"rgb\",\"*\")))))\n",
    "instructions = [{\"instruction_text\": v} for v in insts]\n",
    "sub_data = instruction_cut_clip(instructions, refine=True, append_dot=False, split_func=nltk.sent_tokenize, keep_subs=True)\n",
    "assert len(instructions)==len(sub_data)\n",
    "assert len(instructions)==len(rgbs)\n",
    "\n",
    "train_num = len(rgbs)//2\n",
    "train_indicies = random.sample(list(range(len(rgbs))), train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(sub_data[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(sub_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 27\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msub_instruction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# val\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not list"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "FOLDER_RGB_OUT = Path(\"/root/autodl-tmp/realscene/rgb-vlntj/train\")\n",
    "FOLDER_DEPTH_OUT = Path(\"/root/autodl-tmp/realscene/depth-vlntj/train\")\n",
    "FOLDER_INST_OUT = Path(\"/root/autodl-tmp/realscene/text-vlntj/train\")\n",
    "FOLDER_SUB_OUT = Path(\"/root/autodl-tmp/realscene/sub-vlntj/train\")\n",
    "\n",
    "os.makedirs(FOLDER_RGB_OUT, exist_ok=True)\n",
    "os.makedirs(FOLDER_DEPTH_OUT, exist_ok=True)\n",
    "os.makedirs(FOLDER_INST_OUT, exist_ok=True)\n",
    "os.makedirs(FOLDER_SUB_OUT, exist_ok=True)\n",
    "\n",
    "for i in train_indicies:\n",
    "    rgb_path = FOLDER_RGB_OUT / \"{}.jpg\".format(i)\n",
    "    depth_path = FOLDER_DEPTH_OUT / \"{}.png\".format(i)\n",
    "    inst_path = FOLDER_INST_OUT / \"{}.txt\".format(i)\n",
    "    sub_path = FOLDER_SUB_OUT / \"{}.txt\".format(i)\n",
    "\n",
    "    old_rgb_path = rgbs[i]\n",
    "    old_depth_path = depths[i]\n",
    "    shutil.copy(old_rgb_path, rgb_path)\n",
    "    shutil.copy(old_depth_path, depth_path)\n",
    "    with open(inst_path, \"w\") as f:\n",
    "        f.write(sub_data[i][\"instruction_text\"])\n",
    "    with open(sub_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(sub_data[i][\"sub_instruction\"]))\n",
    "\n",
    "# val\n",
    "from pathlib import Path\n",
    "FOLDER_RGB_OUT = Path(\"/root/autodl-tmp/realscene/rgb-vlntj/val\")\n",
    "FOLDER_DEPTH_OUT = Path(\"/root/autodl-tmp/realscene/depth-vlntj/val\")\n",
    "FOLDER_INST_OUT = Path(\"/root/autodl-tmp/realscene/text-vlntj/val\")\n",
    "FOLDER_SUB_OUT = Path(\"/root/autodl-tmp/realscene/sub-vlntj/val\")\n",
    "\n",
    "os.makedirs(FOLDER_RGB_OUT, exist_ok=True)\n",
    "os.makedirs(FOLDER_DEPTH_OUT, exist_ok=True)\n",
    "os.makedirs(FOLDER_INST_OUT, exist_ok=True)\n",
    "os.makedirs(FOLDER_SUB_OUT, exist_ok=True)\n",
    "\n",
    "for i in range(len(rgbs)):\n",
    "    if i not in train_indicies:\n",
    "        rgb_path = FOLDER_RGB_OUT / \"{}.jpg\".format(i)\n",
    "        depth_path = FOLDER_DEPTH_OUT / \"{}.png\".format(i)\n",
    "        inst_path = FOLDER_INST_OUT / \"{}.txt\".format(i)\n",
    "        sub_path = FOLDER_SUB_OUT / \"{}.txt\".format(i)\n",
    "\n",
    "        old_rgb_path = rgbs[i]\n",
    "        old_depth_path = depths[i]\n",
    "        shutil.copy(old_rgb_path, rgb_path)\n",
    "        shutil.copy(old_depth_path, depth_path)\n",
    "        with open(inst_path, \"w\") as f:\n",
    "            f.write(sub_data[i][\"instruction_text\"])\n",
    "        with open(sub_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(sub_data[i][\"sub_instruction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# vision train\n",
    "FOLDER_RGB_OUT = Path(\"/root/autodl-tmp/realscene/\")\n",
    "FOLDER_DEPTH_OUT = Path(\"/root/autodl-tmp/realscene/\")\n",
    "rgb_files = sorted(list(FOLDER_RGB_OUT.glob(\"rgb-*/val/*.jpg\")))\n",
    "depth_files = sorted(list(FOLDER_DEPTH_OUT.glob(\"depth-*/val/*.png\")))\n",
    "# text train\n",
    "FOLDER_RGB_OUT = Path(\"/root/autodl-tmp/realscene/\")\n",
    "FOLDER_DEPTH_OUT = Path(\"/root/autodl-tmp/realscene/\")\n",
    "text_files = sorted(list(FOLDER_RGB_OUT.glob(\"text-*/val/*.txt\")))\n",
    "sub_files = sorted(list(FOLDER_DEPTH_OUT.glob(\"sub-*/val/*.txt\")))\n",
    "print(len(rgb_files))\n",
    "assert len(rgb_files)==len(depth_files)\n",
    "assert len(text_files)==len(sub_files)\n",
    "for _ in range(10):\n",
    "    idx = random.randint(0, len(rgb_files)-1)\n",
    "    print(\"=========={}==========\".format(idx))\n",
    "    with open(text_files[idx], \"r\") as f:\n",
    "        print(\"inst: \"+f.read())\n",
    "    with open(sub_files[idx], \"r\") as f:\n",
    "        print(\"sub: \"+f.read())\n",
    "    rgb = Image.open(rgb_files[idx])\n",
    "    depth = Image.open(depth_files[idx])\n",
    "    print(rgb_files[idx], depth_files[idx])\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2, 1)\n",
    "    plt.imshow(rgb)\n",
    "    plt.subplot(1,2, 2)\n",
    "    plt.imshow(np.array(depth).astype(float))\n",
    "    plt.show()\n",
    "    # print(np.array(depth).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"/root/autodl-tmp/r2r/\"\n",
    "dataset_path = \"/root/autodl-tmp/r2r/tjark_r2r_all.json\"\n",
    "new_data_path = \"/root/autodl-tmp/r2r_split/\"\n",
    "os.makedirs(new_data_path, exist_ok=True)\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    tjark_r2r = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insts = []\n",
    "for v in tjark_r2r:\n",
    "    if v[\"scan\"]==\"TJARKmeeting603\":\n",
    "        print(v)\n",
    "        insts.append(v[\"instructions\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 100000000000\n",
    "for v in insts:\n",
    "    if len(v)<min_len:\n",
    "        min_len = len(v)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dep = Image.open(\"/root/autodl-tmp/r2r_split/399/depth/3_3.png\")\n",
    "dep = np.array(dep).astype(float)/10000.0\n",
    "dep = np.clip(dep, 0, 1)\n",
    "dep = np.round(dep*255).astype(np.uint8)\n",
    "dep = Image.fromarray(dep)\n",
    "dep.save(\"tmp.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eval = {}\n",
    "import random\n",
    "for i in range(250):\n",
    "    seed = random.randint(0,200000)\n",
    "    os.system(\"cd /root/autodl-tmp; python run.py --run-type eval --exp-config evoenc/config/evoenc_realscene.yaml TASK_CONFIG.SEED {}\".format(seed))\n",
    "    with open(\"/root/autodl-tmp/data/checkpoints/evoenc_p2/evals/stats_ckpt_0_realscene.json\", \"r\") as f:\n",
    "        now_eval = json.loads(f.read())\n",
    "    all_eval[seed] = now_eval\n",
    "with open(\"all_eval.json\", \"w\") as f:\n",
    "    f.write(json.dumps(all_eval, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"all_eval.json\", \"r\") as f:\n",
    "    all_eval = json.loads(f.read())\n",
    "min_loss_rec = [100000, {}]\n",
    "min_loss_mean = [100000, {}]\n",
    "max_acc_inner = [-1, {}]\n",
    "max_acc_outer = [-1, {}]\n",
    "for v in all_eval:\n",
    "    if v[\"losses\"][\"loss_rec\"]<min_loss_rec[0]:\n",
    "        min_loss_rec = [v[\"losses\"][\"loss_rec\"], v]\n",
    "    if v[\"losses\"][\"loss_mean\"]<min_loss_mean[0]:\n",
    "        min_loss_mean = [v[\"losses\"][\"loss_mean\"], v]\n",
    "    if v[\"accuracy\"][\"inner_v\"]>max_acc_inner[0]:\n",
    "        max_acc_inner = [v[\"accuracy\"][\"inner_v\"], v]\n",
    "    if v[\"accuracy\"][\"outer\"]>max_acc_outer[0]:\n",
    "        max_acc_outer = [v[\"accuracy\"][\"outer\"], v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4386"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_indicies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
