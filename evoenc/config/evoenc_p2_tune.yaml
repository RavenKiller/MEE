BASE_TASK_CONFIG_PATH: habitat_extensions/config/vlnce_t.yaml
TRAINER_NAME: real_trainer # dagger, or recollect_trainer
DEBUG: False
SIMULATOR_GPU_ID: 0 # new version
TORCH_GPU_ID: 0
NUM_ENVIRONMENTS: 1
TENSORBOARD_DIR: data/tensorboard_dirs/evoenc_p2_tune
CHECKPOINT_FOLDER: data/checkpoints/evoenc_p2_tune
EVAL_CKPT_PATH_DIR: data/checkpoints/evoenc_p2_tune
RESULTS_DIR: data/checkpoints/evoenc_p2_tune/evals

EVAL:
  USE_CKPT_CONFIG: False
  SPLIT: val_unseen
  EPISODE_COUNT: -1

INFERENCE:
  CKPT_PATH: data/checkpoints/evoenc_p2_tune/ckpt.EEPolicy.37.pth
  SPLIT: val_seen
  FORMAT: r2r

IL:
  epochs: 4
  batch_size: 2
  lr: 1.0e-4
  use_iw: True
  RECOLLECT_TRAINER:
    gt_file:
      data/datasets/R2R_VLNCE_NRSub_T/{split}/{split}_gt.json.gz
  load_from_ckpt: True
  load_from_pretrain: True
  ckpt_to_load: data/checkpoints/evoenc_p2/ckpt.EEPolicy.4.pth
  
  DAGGER:
    iterations: 10
    update_size: 5000
    p: 0.75
    preload_lmdb_features: False
    lmdb_features_dir: data/trajectories_dirs/evoenc_p2_tune/trajectories.lmdb

MODEL:
  policy_name: EEPolicy

  INSTRUCTION_ENCODER:
    bidirectional: True
  PROGRESS_MONITOR:
    use: True
    alpha: 1.0
  PEAK_ATTENTION:
    use: True
    alpha: 0.4
    sigma: 0.6
  DEPTH_ENCODER:
    output_size: 2048 
    single_size: 128
  EVOENC:
    rgb_len: 50
    layers: 4
    inner_dropout: 0.1
  CLIP:
    downsample_size: 7
    trainable: False
